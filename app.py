import io
import json
import os
import re
import traceback
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st
from google import genai

# ==============================================================================
# 1. ìœ í‹¸ë¦¬í‹° & í—¬í¼ í•¨ìˆ˜
# ==============================================================================


def read_excel_sheets(uploaded_file) -> Dict[str, pd.DataFrame]:
    """ì—‘ì…€ íŒŒì¼ì„ ì½ì–´ {sheet_name: df} í˜•íƒœë¡œ ë°˜í™˜ (í—¤ë” ì—†ì´ raw read)"""
    file_bytes = uploaded_file.read()
    uploaded_file.seek(0)
    
    ext = uploaded_file.name.lower().split(".")[-1]
    engine = "openpyxl"
    if ext == "xls":
        engine = "xlrd"

    # header=Noneìœ¼ë¡œ ì½ì–´ì„œ ìœ„ì¹˜ ê¸°ë°˜ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨
    dfs = pd.read_excel(
        io.BytesIO(file_bytes),
        sheet_name=None,
        header=None,
        engine=engine
    )
    return dfs


def get_dataframe_preview_markdown(df: pd.DataFrame, rows: int = 20) -> str:
    """LLMì—ê²Œ ë³´ì—¬ì¤„ DataFrameì˜ Markdown í‘œí˜„"""
    preview_df = df.head(rows).copy()
    # Arrow ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ë¬¸ìì—´ë¡œ ë³€í™˜
    return preview_df.fillna("").astype(str).to_markdown(index=True)


def extract_python_code(text: str) -> str:
    """ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ íŒŒì´ì¬ ì½”ë“œë§Œ ì¶”ì¶œ"""
    if "```python" in text:
        start = text.find("```python") + len("```python")
        end = text.find("```", start)
        if end == -1:
            return text[start:].strip()
        return text[start:end].strip()
    elif "```" in text:
        start = text.find("```") + 3
        end = text.find("```", start)
        if end == -1:
            return text[start:].strip()
        return text[start:end].strip()
    return text.strip()


def extract_json_block(text: str) -> str:
    """ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ í˜¹ì€ í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ"""
    if "```json" in text:
        start = text.find("```json") + len("```json")
        end = text.find("```", start)
        if end == -1:
            return text[start:].strip()
        return text[start:end].strip()
    
    # JSON ë¸”ë¡ì´ ëª…ì‹œì ì´ì§€ ì•Šì€ ê²½ìš° { } ì°¾ê¸°
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1:
        return text[start:end+1]
    return text


def safe_dataframe_display(df: pd.DataFrame, height: int = None):
    """
    st.dataframeì„ ì•ˆì „í•˜ê²Œ ë Œë”ë§í•˜ëŠ” ë˜í¼.
    Arrow Serialization ì—ëŸ¬ ë°œìƒ ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¬ì‹œë„.
    """
    kwargs = {}
    if height is not None:
        kwargs["height"] = height

    try:
        st.dataframe(df, **kwargs)
    except Exception as e:
        st.warning(f"âš ï¸ ê¸°ë³¸ ë°ì´í„°í”„ë ˆì„ ë Œë”ë§ ì‹¤íŒ¨ (Arrow í˜¸í™˜ì„± ë¬¸ì œ). í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ í‘œì‹œí•©ë‹ˆë‹¤. ì—ëŸ¬: {e}")
        try:
            st.dataframe(df.astype(str), **kwargs)
        except Exception as e2:
            st.error(f"âŒ ë°ì´í„° í‘œì‹œ ì‹¤íŒ¨: {e2}")
            st.code(str(df.head(20)))  # ìµœí›„ì˜ ìˆ˜ë‹¨: repr ë¬¸ìì—´ ì¶œë ¥


# ==============================================================================
# 2. LLM ë¡œì§ (Schema ì„¤ê³„ + Code Gen)
# ==============================================================================

def generate_target_schema(api_key: str, all_previews: List[str]) -> Tuple[Dict[str, Any], str, Optional[str]]:
    """
    ì—¬ëŸ¬ íŒŒì¼ì˜ í”„ë¦¬ë·°ë¥¼ ë³´ê³  ê³µí†µ ëª©í‘œ ìŠ¤í‚¤ë§ˆ(Target Schema)ë¥¼ ì œì•ˆ.
    ë°˜í™˜: (schema_dict, raw_response_text, error_message)
    """
    client = genai.Client(api_key=api_key)

    previews_text = "\n\n".join([f"--- File Sample {i+1} ---\n{p}" for i, p in enumerate(all_previews[:3])])

    prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ê°œì˜ ë¹„ì •í˜• ì—‘ì…€ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ì—¬ í•˜ë‚˜ë¡œ í†µí•©í•˜ê¸° ìœ„í•œ 'ê³µí†µ íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆ(Target Schema)'ë¥¼ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤.

ëª©í‘œ:
1. ëª¨ë“  íŒŒì¼ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì¶”ì¶œ ê°€ëŠ¥í•œ í•µì‹¬ ë¶„ì„ í•­ëª©(ì»¬ëŸ¼)ì„ ì •ì˜í•˜ì„¸ìš”.
2. ì»¬ëŸ¼ëª…ì€ ì˜ë¬¸ ìŠ¤ë„¤ì´í¬ ì¼€ì´ìŠ¤(snake_case)ë¡œ í†µì¼í•˜ì„¸ìš”.
3. ê° ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì…ê³¼ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”.

ì…ë ¥ ë°ì´í„° ìƒ˜í”Œ:
{previews_text}

ì‘ë‹µ í˜•ì‹ (JSON):
```json
{{
    "table_name": "integrated_data",
    "columns": [
        {{"name": "region", "type": "string", "description": "ì§€ì—­ëª…"}},
        {{"name": "date", "type": "date", "description": "ê¸°ì¤€ ì¼ì"}},
        {{"name": "population", "type": "int", "description": "ì¸êµ¬ ìˆ˜"}}
    ]
}}
```
ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
    raw_text = ""
    try:
        resp = client.models.generate_content(
            model="gemini-3-pro-preview",
            contents=prompt,
        )
        raw_text = resp.text
        schema = json.loads(extract_json_block(raw_text))
        if not isinstance(schema, dict):
            raise ValueError("LLM ì‘ë‹µì´ dict í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return schema, raw_text, None
    except Exception as e:
        return {"columns": []}, raw_text, str(e)


def generate_transform_code(
    api_key: str,
    file_name: str,
    sheet_name: str,
    df_preview: str,
    target_columns: List[str]
) -> Tuple[str, str, Optional[str]]:
    """
    Raw Data -> Target Schemaë¡œ ë³€í™˜í•˜ëŠ” íŒŒì´ì¬ ì½”ë“œ ì‘ì„±.
    ë°˜í™˜: (code_str, raw_response_text, error_message)
    """
    client = genai.Client(api_key=api_key)

    target_cols_str = ", ".join([f"'{c}'" for c in target_columns])

    system_instruction = f"""
ë‹¹ì‹ ì€ Python Pandas ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
Raw Excel Dataë¥¼ ì •ì œí•˜ì—¬, ë°˜ë“œì‹œ **[Target Schema]**ì— ì •ì˜ëœ ì»¬ëŸ¼ì„ ê°€ì§„ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ëŠ” `transform(df)` í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
1. **í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜**: `def transform(df: pd.DataFrame) -> (pd.DataFrame, dict):`
2. **Target Schema ì¤€ìˆ˜**: ë°˜í™˜ë˜ëŠ” `df_clean`ì€ ë‹¤ìŒ ì»¬ëŸ¼ë“¤ì„ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤: [{target_cols_str}]
   - ë°ì´í„°ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ë‹¤ë©´ `None`ì´ë‚˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ì„¸ìš”.
   - ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ì€ ê³¼ê°íˆ ë²„ë¦¬ì„¸ìš”.
3. **ë©”íƒ€ë°ì´í„°**: ì œëª©, ë‹¨ìœ„ ë“±ì€ ë³„ë„ dictë¡œ ë°˜í™˜.
4. **ì „ì²˜ë¦¬**: 
   - í—¤ë” íƒìƒ‰, ë¶ˆí•„ìš”í•œ ìƒë‹¨ í–‰ ì œê±°
   - 'í•©ê³„', 'ì†Œê³„' ë“± í†µê³„ í–‰ ì œê±°
   - Wide to Long (Melt) ë³€í™˜ ì ê·¹ í™œìš©
   - í—¤ë”ë‚˜ ë°ì´í„° ì‹œì‘ í–‰ì„ ì°¾ì§€ ëª»í•´ë„ `ValueError`ë¥¼ ë˜ì§€ì§€ ë§ê³ , í•©ë¦¬ì ì¸ ê¸°ë³¸ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë¹ˆ DataFrameì´ë¼ë„ ë°˜í™˜í•˜ì„¸ìš”.

### ì¶œë ¥ í˜•ì‹
ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ëŸ­(```python ... ```) ì•ˆì— íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

### ì½”ë“œ í…œí”Œë¦¿
```python
import pandas as pd
import numpy as np

def transform(df):
    metadata = {{}}
    
    # 1. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    # ...
    
    # 2. í—¤ë” ì°¾ê¸° ë° ë°ì´í„° ìŠ¬ë¼ì´ì‹±
    # ...
    
    # 3. ì»¬ëŸ¼ ë§¤í•‘ ë° ë°ì´í„° ì •ì œ
    # ...
    
    # 4. Target Schema ë§ì¶”ê¸° (í•„ìˆ˜ ë‹¨ê³„)
    # í•„ìš”í•œ ì»¬ëŸ¼ ìƒì„± ë° ì„ íƒ
    # df_clean = ...
    
    return df_clean, metadata
```
"""

    prompt = f"""
### ì²˜ë¦¬í•  íŒŒì¼
* íŒŒì¼: {file_name} / ì‹œíŠ¸: {sheet_name}

### ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
```markdown
{df_preview}
```

ìœ„ ë°ì´í„°ë¥¼ [{target_cols_str}] ì»¬ëŸ¼ì„ ê°€ì§„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""

    raw_text = ""
    try:
        resp = client.models.generate_content(
            model="gemini-3-pro-preview",
            contents=system_instruction + "\n" + prompt,
        )
        raw_text = getattr(resp, "text", "") or ""
        if not raw_text.strip():
            raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. API í‚¤/ì¿¼í„° ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        code = extract_python_code(raw_text)
        if not code.strip():
            raise ValueError("LLMì´ ì½”ë“œë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ í”„ë¦¬ë·° í–‰ ìˆ˜ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.")
        return code, raw_text, None
    except Exception as e:
        return "", raw_text, str(e)


# ==============================================================================
# 3. ì½”ë“œ ì‹¤í–‰ê¸°
# ==============================================================================


def execute_user_code(code_str: str, df_raw: pd.DataFrame) -> Tuple[Optional[pd.DataFrame], Optional[Dict], str]:
    """ì‚¬ìš©ì/LLM ì½”ë“œ ì‹¤í–‰ ë˜í¼. ì—ëŸ¬ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜í•œë‹¤."""
    local_scope = {"pd": pd, "df_raw": df_raw, "np": np, "re": re}

    try:
        exec(code_str, globals(), local_scope)
        if "transform" not in local_scope:
            return None, None, "Error: 'transform' í•¨ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        transform_func = local_scope["transform"]
        df_clean, metadata = transform_func(df_raw.copy())

        if not isinstance(df_clean, pd.DataFrame):
            return None, None, "Error: ë°˜í™˜ê°’ì€ DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤."

        return df_clean, metadata, ""

    except Exception:
        return None, None, traceback.format_exc()


# ==============================================================================
# 4. Streamlit UI
# ==============================================================================


def main_app():
    st.set_page_config(page_title="AI í†µí•© ë°ì´í„° ì •ì œê¸°", layout="wide")
    st.title("ğŸ§© AI í†µí•© ë°ì´í„° ì •ì œê¸° (Many to One)")
    st.markdown("""
    ì—¬ëŸ¬ ê°œì˜ **ë¹„ì •í˜• ì—‘ì…€ íŒŒì¼**ì„ AIê°€ ì‘ì„±í•œ ì½”ë“œë¥¼ í†µí•´ **í•˜ë‚˜ì˜ í†µì¼ëœ CSV**ë¡œ í•©ì¹©ë‹ˆë‹¤.
    1. AIê°€ ê³µí†µ ìŠ¤í‚¤ë§ˆ(Target Schema)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
    2. ê° íŒŒì¼ë³„ë¡œ ìŠ¤í‚¤ë§ˆì— ë§ì¶”ëŠ” ë³€í™˜ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    3. ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©í•˜ì—¬ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """)

    # --- ì„¤ì • ---
    st.sidebar.header("ì„¤ì •")
    
    # API í‚¤ ì•ˆë‚´ ë©”ì‹œì§€
    st.sidebar.info("ğŸ’¡ Key ì…ë ¥í•˜ì§€ ì•Šì•„ë„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    api_key = st.sidebar.text_input("Gemini API Key", type="password")
    if not api_key:
        api_key = os.environ.get("GEMINI_API_KEY")
    
    uploaded_files = st.sidebar.file_uploader(
        "ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ)", type=["xls", "xlsx"], accept_multiple_files=True
    )
    st.sidebar.caption("âš ï¸ **ì£¼ì˜**: 'ê°™ì€ ì¢…ë¥˜ì˜ ë°ì´í„°ì§€ë§Œ ì–‘ì‹ì´ ë‹¤ë¥¸ íŒŒì¼'ë§Œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”. ì„±ê²©ì´ ì•„ì˜ˆ ë‹¤ë¥¸ ë°ì´í„°(ì˜ˆ: ì¸êµ¬ìˆ˜ vs ë§¤ì¶œ)ëŠ” ì œì™¸í•´ì•¼ ì •í™•ë„ê°€ ë†’ìŠµë‹ˆë‹¤. ì—¬ëŸ¬ ì‹œíŠ¸ê°€ ìˆëŠ” íŒŒì¼ì¸ ê²½ìš°, í†µì¼í•˜ê³  ì‹¶ì§€ ì•Šì€ ì‹œíŠ¸ê°€ í¬í•¨ëœ íŒŒì¼ì€ ì œì•ˆ/ë³€í™˜ ë‹¨ê³„ì—ì„œ ì œì™¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    if not api_key:
        st.warning("Gemini API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
        
    # Session State ì´ˆê¸°í™”
    if "target_schema" not in st.session_state:
        st.session_state["target_schema"] = {"columns": []}
    if "generated_codes" not in st.session_state:
        st.session_state["generated_codes"] = {}
    if "results" not in st.session_state:
        st.session_state["results"] = {}
    if "llm_logs" not in st.session_state:
        st.session_state["llm_logs"] = []

    # 1. íŒŒì¼ ë¡œë”©
    all_data = {}
    if uploaded_files:
        for file_idx, uf in enumerate(uploaded_files):
            try:
                dfs = read_excel_sheets(uf)
                for sname, df in dfs.items():
                    # (ì—…ë¡œë“œ ìˆœì„œ index, íŒŒì¼ëª…, ì‹œíŠ¸ëª…)ìœ¼ë¡œ í‚¤ë¥¼ ë§Œë“¤ì–´ ë™ì¼ íŒŒì¼ëª… ì¶©ëŒ ë°©ì§€
                    all_data[(file_idx, uf.name, sname)] = df
            except Exception as e:
                st.sidebar.error(f"{uf.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                with st.sidebar.expander("ìƒì„¸ ì—ëŸ¬"):
                    st.code(traceback.format_exc())

    if not all_data:
        st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    # 2. íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆ ì •ì˜
    st.header("1ï¸âƒ£ ê³µí†µ íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆ (Target Schema) ì •ì˜")
    
    # --- ìŠ¤í‚¤ë§ˆ ì„ íƒ ì˜ì—­ (ì „ì²´ í­)
    schema_keys = list(all_data.keys())
    label_map = {k: f"{k[0]+1}) {k[1]}::{k[2]}" for k in schema_keys}
    default_selection = schema_keys[: min(3, len(schema_keys))]
    selected_for_schema = st.multiselect(
        "ìŠ¤í‚¤ë§ˆ ì œì•ˆì— ì‚¬ìš©í•  ì‹œíŠ¸ ì„ íƒ (ì„ íƒ ì—†ìœ¼ë©´ ì „ì²´ ì‚¬ìš©)",
        options=schema_keys,
        default=default_selection,
        format_func=lambda k: label_map.get(k, str(k)),
        help="ê¸´ íŒŒì¼/ì‹œíŠ¸ ì´ë¦„ì„ ì „ë¶€ í‘œì‹œí•˜ê¸° ìœ„í•´ ì˜ì—­ì„ ë„“í˜”ìŠµë‹ˆë‹¤. í•„ìš” ì‹œ ì›í•˜ëŠ” ì‹œíŠ¸ë§Œ ì„ íƒí•˜ì„¸ìš”.",
    )

    col1, col2 = st.columns([1, 2.2])
    with col1:
        if st.button("ğŸ¤– AI ìŠ¤í‚¤ë§ˆ ìë™ ì œì•ˆ", use_container_width=True):
            with st.spinner("ë°ì´í„° ìƒ˜í”Œ ë¶„ì„ ì¤‘..."):
                target_keys = selected_for_schema or schema_keys
                samples = []
                for k in target_keys:
                    df = all_data[k]
                    samples.append(get_dataframe_preview_markdown(df, rows=10))
                
                schema_def, raw_resp, err = generate_target_schema(api_key, samples)
                st.session_state["llm_logs"].append({"type": "schema", "raw": raw_resp, "error": err})

                if err:
                    st.error("ìŠ¤í‚¤ë§ˆ ìƒì„± ì‹¤íŒ¨")
                    st.error(err)
                    with st.expander("AI ì›ë³¸ ì‘ë‹µ"):
                        st.text(raw_resp or "(ë¹ˆ ì‘ë‹µ)")
                else:
                    st.session_state["target_schema"] = schema_def
                    st.success("ìŠ¤í‚¤ë§ˆ ì œì•ˆ ì™„ë£Œ (í¸ì§‘ ê°€ëŠ¥)")

    with col2:
        current_schema = st.session_state.get("target_schema")
        if current_schema is None:
            current_schema = {"columns": []}
            
        schema_text = st.text_area(
            "ìŠ¤í‚¤ë§ˆ ì •ì˜ (JSON í¸ì§‘ ê°€ëŠ¥)", 
            value=json.dumps(current_schema, indent=2, ensure_ascii=False),
            height=240,
        )
        try:
            parsed = json.loads(schema_text)
            if isinstance(parsed, dict):
                st.session_state["target_schema"] = parsed
        except Exception as e:
            st.error(f"JSON í˜•ì‹ ì˜¤ë¥˜: {e}")

    schema = st.session_state.get("target_schema")
    if not isinstance(schema, dict):
        schema = {"columns": []}
    
    target_columns = [c["name"] for c in schema.get("columns", []) if isinstance(c, dict) and "name" in c]
    
    if not target_columns:
        st.warning("íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆì— columnsê°€ ì—†ìŠµë‹ˆë‹¤. JSONì— columns ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        return

    st.success(f"ëª©í‘œ ì»¬ëŸ¼: {target_columns}")

    # 3. ê°œë³„ íŒŒì¼ ë³€í™˜ ì½”ë“œ ìƒì„± ë° ì‹¤í–‰
    st.header("2ï¸âƒ£ íŒŒì¼ë³„ ë³€í™˜ ë° ë³‘í•©")

    entries = list(all_data.items())
    options = [key for key, _ in entries]

    # ì „ì²´ ìë™ ì‹¤í–‰ (ì„ íƒí•œ ì‹œíŠ¸ë§Œ)
    st.markdown("#### âš¡ ì„ íƒ ì‹œíŠ¸ ì¼ê´„ ë³€í™˜")
    auto_run_targets = st.multiselect(
        "ì¼ê´„ ì‹¤í–‰ ëŒ€ìƒ ì‹œíŠ¸ ì„ íƒ",
        options=options,
        default=options,
        format_func=lambda k: f"{k[0]+1}) {k[1]}::{k[2]}",
        help="ì—¬ê¸°ì„œ ì„ íƒí•œ ì‹œíŠ¸ë§Œ ì½”ë“œ ìƒì„±+ì‹¤í–‰ì„ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤."
    )
    auto_run = st.button("ì„ íƒ ì‹œíŠ¸ ì½”ë“œ ìƒì„± + ì‹¤í–‰", help="ì„ íƒí•œ ì‹œíŠ¸ë¥¼ í˜„ì¬ ìŠ¤í‚¤ë§ˆë¡œ ìˆœì°¨ ì‹¤í–‰í•©ë‹ˆë‹¤.")

    valid_dfs = []

    if auto_run:
        run_list = [item for item in entries if item[0] in auto_run_targets]
        if not run_list:
            st.warning("ì¼ê´„ ì‹¤í–‰í•  ì‹œíŠ¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            success, fail = [], []
            progress = st.progress(0)
            total = len(run_list)
            for idx, (key, df_raw) in enumerate(run_list, start=1):
                file_idx, fname, sname = key
                unique_id = f"{file_idx}:{fname}::{sname}"
                try:
                    preview = get_dataframe_preview_markdown(df_raw)
                    code, raw_resp, err = generate_transform_code(api_key, fname, sname, preview, target_columns)
                    st.session_state["llm_logs"].append({"type": "code", "file": fname, "sheet": sname, "raw": raw_resp, "error": err})
                    if err:
                        fail.append((fname, sname, f"ìƒì„± ì‹¤íŒ¨: {err}"))
                        continue

                    st.session_state["generated_codes"][unique_id] = code
                    st.session_state[f"edit_{unique_id}"] = code

                    df_res, meta, exec_err = execute_user_code(code, df_raw)
                    if exec_err:
                        fail.append((fname, sname, f"ì‹¤í–‰ ì˜¤ë¥˜: {exec_err.splitlines()[-1] if exec_err else exec_err}"))
                        continue

                    missing = [c for c in target_columns if c not in df_res.columns]
                    if missing:
                        fail.append((fname, sname, f"ëª©í‘œ ì»¬ëŸ¼ ëˆ„ë½: {missing}"))
                        st.session_state["results"].pop(unique_id, None)
                        continue

                    df_res = df_res[target_columns]
                    df_res["_source_file"] = fname
                    df_res["_source_sheet"] = sname
                    st.session_state["results"][unique_id] = df_res
                    success.append((fname, sname, len(df_res)))
                finally:
                    progress.progress(idx / total)

            if success:
                st.success(f"ìë™ ë³€í™˜ ì„±ê³µ {len(success)}ê±´")
                for f, s, rows in success:
                    st.write(f"âœ… {f} / {s} ({rows}í–‰)")
            if fail:
                st.error(f"ì‹¤íŒ¨ {len(fail)}ê±´")
                for f, s, msg in fail:
                    st.write(f"âŒ {f} / {s}: {msg}")

    # ìˆ˜ë™ ì„ íƒ ì˜ì—­
    selected_key = st.selectbox(
        "ìˆ˜ë™ìœ¼ë¡œ ë³€í™˜í•  íŒŒì¼/ì‹œíŠ¸ ì„ íƒ",
        options=options,
        format_func=lambda k: f"{k[0]+1}) {k[1]}::{k[2]}"
    )

    for key, df_raw in entries:
        file_idx, fname, sname = key
        unique_id = f"{file_idx}:{fname}::{sname}"

        if key != selected_key:
            if unique_id in st.session_state["results"]:
                valid_dfs.append(st.session_state["results"][unique_id])
            continue

        st.subheader(f"ì„ íƒëœ ì‹œíŠ¸: {fname} / {sname}")
        c1, c2 = st.columns([1, 1])

        with c1:
            st.markdown("#### Raw Data Preview")
            safe_dataframe_display(df_raw.head(15))

            if st.button(f"ì½”ë“œ ìƒì„± ({sname})", key=f"gen_{unique_id}"):
                with st.spinner("ë³€í™˜ ì½”ë“œ ì‘ì„± ì¤‘..."):
                    preview = get_dataframe_preview_markdown(df_raw)
                    code, raw_resp, err = generate_transform_code(api_key, fname, sname, preview, target_columns)
                    st.session_state["llm_logs"].append({"type": "code", "file": fname, "sheet": sname, "raw": raw_resp, "error": err})
                    if err:
                        st.error(f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {err}")
                        with st.expander("AI ì›ë³¸ ì‘ë‹µ"):
                            st.text(raw_resp or "(ë¹ˆ ì‘ë‹µ)")
                    else:
                        st.session_state["generated_codes"][unique_id] = code
                        st.session_state[f"edit_{unique_id}"] = code
                        st.success("ì½”ë“œ ìƒì„± ì™„ë£Œ. í•„ìš”í•˜ë©´ ìˆ˜ì • í›„ ì‹¤í–‰í•˜ì„¸ìš”.")

        with c2:
            st.markdown("#### Transformation Code")
            code_key = f"edit_{unique_id}"
            generated_code = st.session_state["generated_codes"].get(unique_id, "")
            if code_key not in st.session_state and generated_code:
                st.session_state[code_key] = generated_code
            edited_code = st.text_area("Python Code", st.session_state.get(code_key, generated_code), height=300, key=code_key)
            if not edited_code.strip():
                st.info("ì½”ë“œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ 'ì½”ë“œ ìƒì„±'ì„ ë‹¤ì‹œ ëˆŒëŸ¬ì£¼ì„¸ìš”. ì—ëŸ¬ê°€ ìˆë‹¤ë©´ ì•„ë˜ LLM ì‘ë‹µ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            st.session_state["generated_codes"][unique_id] = edited_code

            if st.button(f"ì‹¤í–‰ ({sname})", key=f"exec_{unique_id}"):
                df_res, meta, err = execute_user_code(edited_code, df_raw)
                if err:
                    st.error("ì½”ë“œ ì‹¤í–‰ ì˜¤ë¥˜")
                    st.code(err, language="text")
                else:
                    missing = [c for c in target_columns if c not in df_res.columns]
                    if missing:
                        st.warning(f"âš ï¸ ëª©í‘œ ì»¬ëŸ¼ ëˆ„ë½ -> {missing}")
                        st.session_state["results"].pop(unique_id, None)
                    else:
                        df_res = df_res[target_columns]
                        df_res["_source_file"] = fname
                        df_res["_source_sheet"] = sname
                        st.session_state["results"][unique_id] = df_res
                        st.success("ë³€í™˜ ì„±ê³µ! ì•„ë˜ì—ì„œ ë³‘í•© ê°€ëŠ¥")
                        safe_dataframe_display(df_res.head(5))

        if unique_id in st.session_state["results"]:
            valid_dfs.append(st.session_state["results"][unique_id])

    # ì§„í–‰ í˜„í™© ìš”ì•½
    st.divider()
    st.markdown("### ì§„í–‰ í˜„í™©")
    status_rows = []
    for (idx, fname, sname) in all_data.keys():
        uid = f"{idx}:{fname}::{sname}"
        status_rows.append({
            "#": idx + 1,
            "íŒŒì¼": fname,
            "ì‹œíŠ¸": sname,
            "ì½”ë“œ ìƒì„±": "âœ…" if uid in st.session_state["generated_codes"] and st.session_state["generated_codes"][uid].strip() else "â¬œ",
            "ì‹¤í–‰ ì™„ë£Œ": "âœ…" if uid in st.session_state["results"] else "â¬œ",
        })
    st.dataframe(pd.DataFrame(status_rows))

    # 4. ìµœì¢… ë³‘í•© ë° ë‹¤ìš´ë¡œë“œ
    st.divider()
    st.header("3ï¸âƒ£ ìµœì¢… ë³‘í•© ë° ë‹¤ìš´ë¡œë“œ")
    
    if valid_dfs:
        try:
            final_df = pd.concat(valid_dfs, ignore_index=True)
            st.markdown(f"### ğŸ“¦ ì´ {len(valid_dfs)}ê°œ íŒŒì¼ ë³‘í•© ì™„ë£Œ ({len(final_df)} í–‰)")
            safe_dataframe_display(final_df.head(20))
            
            csv_bytes = final_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                label="ğŸ“¥ í†µí•© CSV ë‹¤ìš´ë¡œë“œ",
                data=csv_bytes,
                file_name="merged_data.csv",
                mime="text/csv"
            )
        except Exception as e:
            st.error("ë³‘í•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            st.code(traceback.format_exc())
    else:
        st.info("ì•„ì§ ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê° íƒ­ì—ì„œ ì½”ë“œë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

    # ë””ë²„ê·¸: LLM ì›ë³¸ ì‘ë‹µ ëª¨ì•„ë³´ê¸°
    with st.expander("LLM ì‘ë‹µ ë¡œê·¸"):
        if not st.session_state["llm_logs"]:
            st.write("ë¡œê·¸ ì—†ìŒ")
        else:
            for i, log in enumerate(reversed(st.session_state["llm_logs"])):
                label = f"{i+1}. {log.get('type', '')} | {log.get('file', '')} {log.get('sheet', '')}"
                st.markdown(f"**{label}**")
                if log.get("error"):
                    st.error(log["error"])
                st.code(log.get("raw", "(raw ì—†ìŒ)"))


def main():
    try:
        main_app()
    except Exception as e:
        st.error("ğŸš¨ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        st.error(str(e))
        with st.expander("ìƒì„¸ ì˜¤ë¥˜ ë¡œê·¸ (Traceback)"):
            st.code(traceback.format_exc())


if __name__ == "__main__":
    main()
