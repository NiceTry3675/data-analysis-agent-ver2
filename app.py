import io
import json
import os
from datetime import datetime
from typing import Any, Dict, List, Tuple

import pandas as pd
import streamlit as st
from google import genai


# ======================================
# 1. ì—‘ì…€ ì½ê¸° & ì‹œíŠ¸ í”„ë¡œíŒŒì¼ ìƒì„±
# ======================================


def read_all_sheets(uploaded_file) -> Dict[str, pd.DataFrame]:
    """
    Streamlit UploadedFile â†’ {sheet_name: DataFrame(header=None)} ë¡œ ì½ê¸°.
    """
    file_bytes = uploaded_file.read()
    uploaded_file.seek(0)

    ext = uploaded_file.name.lower().split(".")[-1]
    engine = "openpyxl"
    if ext == "xls":
        engine = "xlrd"  # êµ¬ë²„ì „ xls ìš©

    dfs = pd.read_excel(
        io.BytesIO(file_bytes),
        sheet_name=None,
        header=None,
        engine=engine,
    )
    return dfs


def profile_sheet_for_llm(
    df_raw: pd.DataFrame,
    file_name: str,
    sheet_name: str,
    max_rows: int = 12,
    max_cols: int = 12,
    sample_per_col: int = 5,
) -> Dict[str, Any]:
    """
    LLMì— ë„˜ê¸¸ ì‹œíŠ¸ í”„ë¡œíŒŒì¼:
    - ìƒìœ„ Ní–‰ ê°’ (í…ìŠ¤íŠ¸)
    - ì»¬ëŸ¼ë³„ íƒ€ì…/ìƒ˜í”Œ ìš”ì•½
    """
    n_rows, n_cols = df_raw.shape
    preview_rows: List[List[str]] = []
    for i in range(min(max_rows, n_rows)):
        row: List[str] = []
        for j in range(min(max_cols, n_cols)):
            val = df_raw.iat[i, j]
            if pd.isna(val):
                row.append("")
            else:
                row.append(str(val))
        preview_rows.append(row)

    columns_profile: List[Dict[str, Any]] = []
    for col_idx in range(min(max_cols, n_cols)):
        col = df_raw.iloc[:, col_idx]
        non_null = col.dropna()
        head_samples = non_null.head(sample_per_col).astype(str).tolist()
        unique_ratio = float(non_null.nunique() / non_null.size) if non_null.size > 0 else 0.0

        if pd.api.types.is_numeric_dtype(col):
            logical_type = "numeric"
        elif pd.api.types.is_datetime64_any_dtype(col):
            logical_type = "datetime"
        else:
            logical_type = "string"

        columns_profile.append(
            {
                "index": col_idx,
                "pandas_dtype": str(col.dtype),
                "logical_type_guess": logical_type,
                "non_null_ratio": float(non_null.size / len(col)) if len(col) > 0 else 0.0,
                "unique_ratio": unique_ratio,
                "sample_values": head_samples,
            }
        )

    return {
        "file_name": file_name,
        "sheet_name": sheet_name,
        "n_rows": int(n_rows),
        "n_cols": int(n_cols),
        "preview_rows": preview_rows,
        "columns": columns_profile,
    }


# ======================================
# 2. LLM í”„ë¡¬í”„íŠ¸ & í˜¸ì¶œ
# ======================================


def build_llm_prompt(sheet_profiles: List[Dict[str, Any]]) -> str:
    """
    LLMì—ê²Œ:
      - target_tables (í†µì¼ ìŠ¤í‚¤ë§ˆ)
      - sheet_mappings (ì‹œíŠ¸ë³„ ì „ì²˜ë¦¬/ë§¤í•‘/ë©”íƒ€ë°ì´í„° ì¶”ì¶œ)
    ì„ ì„¤ê³„í•˜ê²Œ í•˜ëŠ” í”„ë¡¬í”„íŠ¸.
    """

    profiles_json = json.dumps(sheet_profiles, ensure_ascii=False, indent=2)

    dsl_spec = r"""
ë‹¹ì‹ ì€ ì—‘ì…€ ë¦¬í¬íŠ¸ë¥¼ ì •ê·œí™”í•˜ì—¬ ê¹”ë”í•œ CSV + ë©”íƒ€ë°ì´í„°(JSON)ë¡œ ë§Œë“œëŠ” ë°ì´í„° ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.

ëª©í‘œ:
- CSVì—ëŠ” ë¶„ì„ì— í•„ìš”í•œ **í•µì‹¬ í•„ë“œë§Œ** ë‹´ìŠµë‹ˆë‹¤.
- CSVë¡œ í‘œí˜„í•˜ê¸° ì§€ì €ë¶„í•œ ì •ë³´(ë©€í‹° í—¤ë”, ì œëª©, ê¸°ì¤€ì¼, ë‹¨ìœ„, ì£¼ì„ ë“±)ëŠ” ë©”íƒ€ë°ì´í„°(JSON)ë¡œ ëºë‹ˆë‹¤.
- ì—¬ëŸ¬ ë‹¤ë¥¸ í˜•ì‹ì˜ ì‹œíŠ¸ë¼ë„, ê°€ëŠ¥í•œ í•œ **ê³µí†µ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ**ë¡œ í†µì¼í•©ë‹ˆë‹¤.

ì…ë ¥:
- ì—¬ëŸ¬ ê°œì˜ ì—‘ì…€ íŒŒì¼/ì‹œíŠ¸ì— ëŒ€í•´, ìƒìœ„ ì¼ë¶€ í–‰ê³¼ ì»¬ëŸ¼ ìš”ì•½ì„ ë‹´ì€ í”„ë¡œíŒŒì¼ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤.

ì¶œë ¥:
- 1) target_tables: í†µì¼ëœ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜
- 2) sheet_mappings: ê° (file_name, sheet_name)ì„ ì–´ë–¤ í…Œì´ë¸”ë¡œ, ì–´ë–»ê²Œ ë³€í™˜í• ì§€ ê·œì¹™ ì •ì˜

### 1) target_tables êµ¬ì¡°

```json
"target_tables": [
  {
    "name": "string (í…Œì´ë¸” ì´ë¦„, ì˜ˆ: population_main)",
    "description": "ì´ í…Œì´ë¸”ì´ ë¬´ì—‡ì„ í‘œí˜„í•˜ëŠ”ì§€ í•œêµ­ì–´ ì„¤ëª…",
    "columns": [
      {
        "name": "string (ì˜ë¬¸ ìŠ¤ë„¤ì´í¬ì¼€ì´ìŠ¤ ê¶Œì¥, ì˜ˆ: region_name)",
        "dtype": "string | int | float | bool | date",
        "role": "data | metadata | both",
        "description": "ì»¬ëŸ¼ ì˜ë¯¸ (í•œêµ­ì–´)"
      }
    ]
  }
]
```

* roleì´ ì˜ë¯¸í•˜ëŠ” ê²ƒ:

  * "data": CSVì— í¬í•¨ (ë¶„ì„ìš© í•µì‹¬ ë°ì´í„°)
  * "metadata": metadata.jsonì—ë§Œ í¬í•¨ (í–‰ë§ˆë‹¤ ë°˜ë³µí•˜ê¸° ì• ë§¤í•œ ì •ë³´)
  * "both": CSVì—ë„ ì»¬ëŸ¼ìœ¼ë¡œ ë‘ê³ , metadata.jsonì—ë„ ìš”ì•½/ì„¤ëª…ì— í¬í•¨

### 2) sheet_mappings êµ¬ì¡°

```json
"sheet_mappings": [
  {
    "file_name": "ì›ë³¸ ì—‘ì…€ íŒŒì¼ëª… (í”„ë¡œíŒŒì¼ì— ë‚˜ì˜¨ ê°’ê³¼ ë™ì¼í•˜ê²Œ)",
    "sheet_name": "ì›ë³¸ ì‹œíŠ¸ëª… (í”„ë¡œíŒŒì¼ì— ë‚˜ì˜¨ ê°’ê³¼ ë™ì¼í•˜ê²Œ)",
    "table_name": "target_tables ì¤‘ í•˜ë‚˜ì˜ name",

    "preprocess": {
      "header_row": 5,               // 0-based, ì´ í–‰ì„ ì»¬ëŸ¼ í—¤ë”ë¡œ ì‚¬ìš©
      "drop_top_rows": 0,            // header_row ë°”ë¡œ ì•„ë˜ì—ì„œ ì¶”ê°€ë¡œ ë²„ë¦´ í–‰ ìˆ˜
      "drop_bottom_rows": 0,         // ë§ˆì§€ë§‰ì—ì„œ ëª‡ í–‰ì„ ë²„ë¦´ì§€
      "drop_empty_rows": true,       // ì „ë¶€ ë¹„ì–´ìˆëŠ” í–‰ drop ì—¬ë¶€
      "drop_empty_columns": true,    // ì „ë¶€ ë¹„ì–´ìˆëŠ” ì—´ drop ì—¬ë¶€
      "drop_rows_matching": {
        "column_index": 0,           // ì²« ë²ˆì§¸ ì—´ ê¸°ì¤€
        "equals": ["í•©ê³„", "ì „ì›”í•©ê³„"] // ì´ ê°’(ë˜ëŠ” í…ìŠ¤íŠ¸)ì¸ í–‰ì€ ë°ì´í„°ì—ì„œ ì œì™¸
      }
    },

    "melt": {
      "enabled": true,
      "id_columns": ["ì—°ë ¹", "ì„±ë³„"],        // header ì ìš© í›„ ê¸°ì¤€ì´ ë˜ëŠ” id ì»¬ëŸ¼ë“¤
      "value_columns": "all_except_id",      // ë˜ëŠ” ["ì§„ì£¼ì‹œ", "ë¬¸ì‚°ì", ...]
      "variable_name": "ì§€ì—­",               // melt í›„ ì§€ì—­ ì´ë¦„ì´ ë“¤ì–´ê°ˆ ì»¬ëŸ¼ëª…
      "value_name": "ì¸êµ¬"                   // melt í›„ ê°’ì´ ë“¤ì–´ê°ˆ ì»¬ëŸ¼ëª…
    },

    "column_mapping": {
      "ì—°ë ¹": "age_label",      // í˜„ì¬ ì»¬ëŸ¼ëª…(ë˜ëŠ” melt í›„ ì´ë¦„) -> target_tables ì»¬ëŸ¼ëª…
      "ì„±ë³„": "gender",
      "ì§€ì—­": "region_name",
      "ì¸êµ¬": "population",
      "í†µê³„ì—°ì›”": "base_date"   // í•„ìš”í•˜ë©´ ì´ë ‡ê²Œ í—¤ë” í–‰ì— ìˆëŠ” ê²ƒë„ ë§¤í•‘ ê°€ëŠ¥
    },

    "column_roles_override": {
      "base_date": "metadata"   // target_tables.columns.role ë¥¼ ë®ì–´ì“°ê³  ì‹¶ì„ ë•Œ
    },

    "metadata_cells": [
      {
        "field": "base_date",   // target_tables.columns.name ì¤‘ metadata/both ë¡œ ì„¤ì •ëœ í•„ë“œ
        "row": 1,               // ì›ë³¸ df_raw ê¸°ì¤€ 0-based
        "col": 0,
        "parse_hint": "date_in_text"  // ì„ íƒ: "date_in_text" ë“±, ì‚¬ëŒì´ ë³´ë©´ ì´í•´ ê°€ëŠ¥í•œ íŒíŠ¸
      }
    ],

    "drop_unmapped_columns": true
  }
]
```

ì£¼ì˜:

* file_name / sheet_name ì€ ë°˜ë“œì‹œ **í”„ë¡œíŒŒì¼ì— ë‚˜ì˜¨ ë¬¸ìì—´ê³¼ ë™ì¼í•˜ê²Œ** ì¨ì•¼ í•©ë‹ˆë‹¤.
* í•„ìš” ì—†ê±°ë‚˜ í†µì¼ì´ ì•ˆ ë˜ëŠ” ì •ë³´ëŠ”:

  * target_tables ì— ì»¬ëŸ¼ì„ ë§Œë“¤ì§€ ë§ê³ ,
  * sheet_mappings ì—ì„œë„ column_mapping ì— í¬í•¨ì‹œí‚¤ì§€ ë§ˆì‹­ì‹œì˜¤.
* CSVì— ë„£ê¸°ì—ëŠ” ì• ë§¤í•˜ê±°ë‚˜ ë°˜ë³µì´ ì˜ë¯¸ ì—†ëŠ” ì •ë³´(ì œëª©, í†µê³„ ê¸°ì¤€ì¼, ë‹¨ìœ„ ë“±)ëŠ”:

  * target_tables.columns ì— role="metadata" ë¡œ ì •ì˜í•˜ê³ ,
  * metadata_cellsë¡œ ì¶”ì¶œ ê·œì¹™ì„ ì§€ì •í•˜ì„¸ìš”.

### 3) ìµœì¢… ì‘ë‹µ í˜•ì‹

ë°˜ë“œì‹œ ì•„ë˜ êµ¬ì¡°ì˜ JSON **í•˜ë‚˜ë§Œ** ì¶œë ¥í•˜ì„¸ìš”:

```json
{
  "target_tables": [ ... ],
  "sheet_mappings": [ ... ]
}
```

ë§ˆí¬ë‹¤ìš´, ì£¼ì„, ì„¤ëª… ë¬¸ì¥ ë“±ì€ JSON ë°”ê¹¥ì— ì ˆëŒ€ ì“°ì§€ ë§ˆì‹­ì‹œì˜¤.
ì„¤ëª… í…ìŠ¤íŠ¸(ì»¬ëŸ¼ description ë“±)ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•´ë„ ë©ë‹ˆë‹¤.
"""

    prompt = f"""{dsl_spec}

ì•„ë˜ëŠ” ì´ë²ˆì— ì²˜ë¦¬í•´ì•¼ í•  ì—‘ì…€ ì‹œíŠ¸ë“¤ì˜ í”„ë¡œíŒŒì¼ì…ë‹ˆë‹¤:

<sheet_profiles>
{profiles_json}
</sheet_profiles>

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš”êµ¬ëœ JSON í˜•ì‹ì— ì •í™•íˆ ë§ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""
    return prompt


def extract_json_object(text: str) -> str:
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1:
        raise ValueError("ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return text[start : end + 1]


def call_gemini_for_spec(
    api_key: str, sheet_profiles: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Gemini 3 Pro Preview í˜¸ì¶œ â†’ target_tables + sheet_mappings spec ë°˜í™˜
    """
    client = genai.Client(api_key=api_key)
    prompt = build_llm_prompt(sheet_profiles)

    resp = client.models.generate_content(
        model="gemini-3-pro-preview",
        contents=prompt,
    )

    text = resp.text
    json_str = extract_json_object(text)
    spec = json.loads(json_str)
    return spec


# ======================================
# 3. ê·œì¹™ ì‹¤í–‰ê¸° (Executor)
# ======================================


def apply_mapping_to_sheet(
    df_raw: pd.DataFrame,
    file_name: str,
    sheet_name: str,
    mapping: Dict[str, Any],
    table_def: Dict[str, Any],
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    í•˜ë‚˜ì˜ (file, sheet)ì— ëŒ€í•´:
    - preprocess ê·œì¹™ ì ìš©
    - í•„ìš” ì‹œ melt
    - column_mappingìœ¼ë¡œ canonical ì»¬ëŸ¼ ì´ë¦„ ë¶€ì—¬
    - target_table ì •ì˜ì— ë§ì¶° canonical ì»¬ëŸ¼ë§Œ ìœ ì§€
    - metadata_cells ë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

    ë°˜í™˜:
    - df_canon: canonical ì»¬ëŸ¼ + _source_file/_source_sheet
    - sheet_metadata: {"field_name": value, ...}
    """
    df = df_raw.copy()
    preprocess = mapping.get("preprocess", {})
    header_row = preprocess.get("header_row")
    drop_top = int(preprocess.get("drop_top_rows", 0) or 0)
    drop_bottom = int(preprocess.get("drop_bottom_rows", 0) or 0)
    drop_empty_rows = bool(preprocess.get("drop_empty_rows", True))
    drop_empty_cols = bool(preprocess.get("drop_empty_columns", True))
    drop_rows_matching = preprocess.get("drop_rows_matching", {})

    n_rows, n_cols = df.shape
    if header_row is None:
        header_row = 0
    header_row = max(0, min(int(header_row), n_rows - 1))

    # header_rowë¶€í„° ì˜ë¼ì„œ í—¤ë” ì ìš©
    df = df.iloc[header_row:, :].copy()
    if df.empty:
        return pd.DataFrame(), {}

    header = df.iloc[0].fillna("").astype(str).str.strip()
    df = df.iloc[1:, :].copy()
    df.columns = header

    # ì¶”ê°€ ìƒë‹¨ í–‰ ì œê±°
    if drop_top > 0 and drop_top < len(df):
        df = df.iloc[drop_top:, :].copy()

    # í•˜ë‹¨ í–‰ ì œê±°
    if drop_bottom > 0 and drop_bottom < len(df):
        df = df.iloc[:-drop_bottom, :].copy()

    # ë¹ˆ í–‰/ì—´ ì œê±°
    if drop_empty_rows:
        df = df.dropna(how="all")
    if drop_empty_cols:
        df = df.dropna(how="all", axis=1)
    df = df.reset_index(drop=True)

    # íŠ¹ì • ê°’(ì˜ˆ: 'í•©ê³„')ì¸ í–‰ ì œê±°
    if drop_rows_matching:
        col_idx = drop_rows_matching.get("column_index")
        equals_vals = drop_rows_matching.get("equals", [])
        if col_idx is not None and equals_vals:
            col_idx = int(col_idx)
            if 0 <= col_idx < len(df.columns):
                col_name = df.columns[col_idx]
                mask = df[col_name].astype(str).isin([str(v) for v in equals_vals])
                df = df[~mask].copy()
                df = df.reset_index(drop=True)

    # melt ë‹¨ê³„ (wide â†’ long ë³€í™˜)
    melt_spec = mapping.get("melt")
    if melt_spec and melt_spec.get("enabled"):
        id_cols_spec = melt_spec.get("id_columns", [])
        id_cols = [c for c in id_cols_spec if c in df.columns]
        value_cols_spec = melt_spec.get("value_columns", "all_except_id")

        if isinstance(value_cols_spec, list):
            value_cols = [c for c in value_cols_spec if c in df.columns]
        else:
            value_cols = [c for c in df.columns if c not in id_cols]

        var_name = melt_spec.get("variable_name", "variable")
        val_name = melt_spec.get("value_name", "value")

        if value_cols:
            df = pd.melt(
                df,
                id_vars=id_cols,
                value_vars=value_cols,
                var_name=var_name,
                value_name=val_name,
            )

    # column_mapping ì ìš© â†’ canonical ì»¬ëŸ¼ëª…ìœ¼ë¡œ rename
    col_map = mapping.get("column_mapping", {})
    safe_map = {src: dst for src, dst in col_map.items() if src in df.columns}
    df = df.rename(columns=safe_map)

    # target_table ì •ì˜ ê¸°ë°˜ canonical ì»¬ëŸ¼ ë§Œë“¤ê¸°
    canonical_cols = [c["name"] for c in table_def.get("columns", [])]
    for col in canonical_cols:
        if col not in df.columns:
            df[col] = None

    # canonical ì»¬ëŸ¼ë§Œ ìœ ì§€
    df_canon = df[canonical_cols].copy()

    # ì›ë³¸ ì¶œì²˜ ì •ë³´ ì¶”ê°€ (CSVì—ëŠ” í•­ìƒ í¬í•¨í•  ê²ƒ)
    df_canon["_source_file"] = file_name
    df_canon["_source_sheet"] = sheet_name

    # metadata ì…€ ì¶”ì¶œ (ì›ë³¸ df_raw ê¸°ì¤€)
    metadata_cells = mapping.get("metadata_cells", [])
    sheet_meta_fields: Dict[str, Any] = {}
    for m in metadata_cells:
        field = m.get("field")
        row = m.get("row")
        col = m.get("col")
        if field is None or row is None or col is None:
            continue
        try:
            val = df_raw.iloc[int(row), int(col)]
        except Exception:
            val = None
        if pd.isna(val):
            val = None
        if val is not None:
            sheet_meta_fields[field] = str(val)

    return df_canon, sheet_meta_fields


def normalize_all_files(
    all_files: List[Dict[str, Any]], spec: Dict[str, Any]
) -> Tuple[Dict[str, Dict[str, pd.DataFrame]], Dict[str, Any]]:
    """
    spec(target_tables + sheet_mappings)ì„ ì‚¬ìš©í•´ì„œ
    - íŒŒì¼ë³„/í…Œì´ë¸”ë³„ canonical DataFrame ìƒì„±
    - metadata.jsonì— ë“¤ì–´ê°ˆ summary ìƒì„±
    """
    target_tables = {t["name"]: t for t in spec.get("target_tables", [])}
    sheet_mappings = spec.get("sheet_mappings", [])

    # (file_name, sheet_name) -> [mappings]
    mapping_index: Dict[Tuple[str, str], List[Dict[str, Any]]] = {}
    for m in sheet_mappings:
        fname = m.get("file_name")
        sname = m.get("sheet_name")
        if not fname or not sname:
            continue
        mapping_index.setdefault((fname, sname), []).append(m)

    # ê²°ê³¼ êµ¬ì¡°
    outputs: Dict[str, Dict[str, pd.DataFrame]] = {}  # file -> table -> df
    tables_summary: Dict[str, Any] = {}  # table_name -> {columns, instances: [...]}

    for tname, tdef in target_tables.items():
        tables_summary[tname] = {
            "columns": tdef.get("columns", []),
            "instances": [],  # ê° íŒŒì¼/ì‹œíŠ¸ë³„ {file_name, sheet_name, row_count, metadata_fields}
        }

    for file_entry in all_files:
        fname = file_entry["file_name"]
        sheets: Dict[str, pd.DataFrame] = file_entry["sheets"]
        outputs[fname] = {}

        for sheet_name, df_raw in sheets.items():
            key = (fname, sheet_name)
            if key not in mapping_index:
                continue

            for mapping in mapping_index[key]:
                table_name = mapping.get("table_name")
                if table_name not in target_tables:
                    continue
                table_def = target_tables[table_name]

                df_canon, meta_fields = apply_mapping_to_sheet(
                    df_raw, fname, sheet_name, mapping, table_def
                )
                if df_canon.empty:
                    continue

                # outputsì— append
                if table_name not in outputs[fname]:
                    outputs[fname][table_name] = df_canon
                else:
                    outputs[fname][table_name] = pd.concat(
                        [outputs[fname][table_name], df_canon], ignore_index=True
                    )

                # metadata summary
                tables_summary[table_name]["instances"].append(
                    {
                        "file_name": fname,
                        "sheet_name": sheet_name,
                        "row_count": int(df_canon.shape[0]),
                        "metadata_fields": meta_fields,
                    }
                )

    # metadata.jsonì˜ ìƒìœ„ êµ¬ì¡°
    metadata = {
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "llm_model": "gemini-3-pro-preview",
        "target_tables": spec.get("target_tables", []),
        "sheet_mappings": spec.get("sheet_mappings", []),
        "tables": tables_summary,
    }

    return outputs, metadata


# ======================================
# 4. Streamlit UI
# ======================================


def main():
    st.set_page_config(page_title="LLM ê¸°ë°˜ Excel â†’ í†µì¼ CSV + ë©”íƒ€ë°ì´í„°", layout="wide")
    st.title("ğŸ§¹ LLM ê¸°ë°˜ Excel ì •ë¦¬ê¸°: í†µì¼ëœ CSV + metadata.json")

    st.markdown(
        """
**ëª©í‘œ**

* ì—‘ì…€ ë³´ê³ ì„œ(ë³µì¡í•œ í—¤ë”/ì£¼ì„/ì„œì‹)ë¥¼

  * ê¹”ë”í•œ í†µì¼ ìŠ¤í‚¤ë§ˆì˜ CSVë¡œ ë§Œë“¤ê³ ,
  * CSVë¡œ ë„£ê¸° ì• ë§¤í•œ ì •ë³´ëŠ” metadata.jsonìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
* ìŠ¤í‚¤ë§ˆ/ê·œì¹™ì€ ì‚¬ëŒì´ í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê³ , **LLM(Gemini 3 Pro Preview)ê°€ ìŠ¤ìŠ¤ë¡œ ì„¤ê³„**í•©ë‹ˆë‹¤.
* íŒŒì´ì¬ ì½”ë“œëŠ” ê·¸ ìŠ¤í™ì„ ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ëŠ” **ì¼ë°˜í™”ëœ ì‹¤í–‰ê¸°**ì…ë‹ˆë‹¤.
        """
    )

    st.sidebar.header("Gemini ì„¤ì •")
    api_key_input = st.sidebar.text_input(
        "GEMINI_API_KEY",
        type="password",
        help="Google AI Studio / Gemini API í‚¤. í™˜ê²½ë³€ìˆ˜ GEMINI_API_KEYë¡œë„ ì„¤ì • ê°€ëŠ¥.",
    )
    api_key_env = os.environ.get("GEMINI_API_KEY")
    api_key = api_key_input or api_key_env

    if not api_key:
        st.sidebar.warning("LLM ê¸°ëŠ¥ì„ ì“°ë ¤ë©´ GEMINI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    uploaded_files = st.file_uploader(
        "ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ (.xls, .xlsx) â€” ì—¬ëŸ¬ ê°œ ê°€ëŠ¥",
        type=["xls", "xlsx"],
        accept_multiple_files=True,
    )

    if not uploaded_files:
        st.info("ë¨¼ì € ì—‘ì…€ íŒŒì¼ì„ í•˜ë‚˜ ì´ìƒ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        return

    # 1) ì—‘ì…€ ë¡œë“œ + ì‹œíŠ¸ í”„ë¡œíŒŒì¼ ìƒì„±
    all_files: List[Dict[str, Any]] = []
    all_sheet_profiles: List[Dict[str, Any]] = []

    st.subheader("1ï¸âƒ£ ì—…ë¡œë“œëœ íŒŒì¼ & ì‹œíŠ¸ êµ¬ì¡° í™•ì¸")

    for f in uploaded_files:
        st.markdown(f"#### ğŸ“ {f.name}")
        try:
            sheets = read_all_sheets(f)
        except Exception as e:
            st.error(f"{f.name} ì½ê¸° ì‹¤íŒ¨: {e}")
            continue

        file_entry = {"file_name": f.name, "sheets": sheets}
        all_files.append(file_entry)

        st.write("ì‹œíŠ¸ ëª©ë¡:", list(sheets.keys()))

        for sheet_name, df_raw in sheets.items():
            prof = profile_sheet_for_llm(df_raw, f.name, sheet_name)
            all_sheet_profiles.append(prof)

            with st.expander(f"ì‹œíŠ¸ í”„ë¡œíŒŒì¼: {sheet_name}"):
                st.json(prof)

    # 2) LLMìœ¼ë¡œ ìŠ¤í‚¤ë§ˆ/ê·œì¹™ ìƒì„±
    st.subheader("2ï¸âƒ£ LLMìœ¼ë¡œ í†µì¼ ìŠ¤í‚¤ë§ˆ + ì‹œíŠ¸ë³„ ë³€í™˜ ê·œì¹™ ìƒì„±")

    spec_state_key = "llm_transform_spec"
    spec_container = st.empty()

    if st.button("Gemini 3 Pro Previewë¡œ ìŠ¤í‚¤ë§ˆ/ê·œì¹™ ì„¤ê³„ ìš”ì²­"):
        if not api_key:
            st.error("GEMINI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            with st.spinner("Gemini 3 Pro Previewì—ê²Œ ì„¤ê³„ ìš”ì²­ ì¤‘..."):
                try:
                    spec = call_gemini_for_spec(api_key, all_sheet_profiles)
                    st.session_state[spec_state_key] = spec
                    st.success("LLM transform spec ìƒì„± ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"LLM í˜¸ì¶œ/íŒŒì‹± ì‹¤íŒ¨: {e}")

    spec = st.session_state.get(spec_state_key)
    if spec:
        spec_container.subheader("LLM transform spec (ìš”ì•½)")
        spec_container.code(
            json.dumps(spec, ensure_ascii=False, indent=2)[:6000], language="json"
        )
    else:
        spec_container.info("ì•„ì§ LLM transform specì´ ì—†ìŠµë‹ˆë‹¤. ìœ„ ë²„íŠ¼ìœ¼ë¡œ ìƒì„±í•´ ì£¼ì„¸ìš”.")

    # 3) ê·œì¹™ ì‹¤í–‰ â†’ CSV + metadata.json
    st.subheader("3ï¸âƒ£ ê·œì¹™ ì‹¤í–‰ â†’ í†µì¼ëœ CSV + metadata.json ìƒì„±")

    if st.button("ê·œì¹™ ì‹¤í–‰ ë° ê²°ê³¼ ìƒì„±"):
        if not spec:
            st.error("ë¨¼ì € LLMìœ¼ë¡œë¶€í„° transform specì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
        else:
            with st.spinner("ê·œì¹™ ì‹¤í–‰ ì¤‘..."):
                outputs, metadata = normalize_all_files(all_files, spec)

                st.success("ë³€í™˜ ì™„ë£Œ! ì•„ë˜ì—ì„œ CSVì™€ metadata.jsonì„ í™•ì¸/ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                # í…Œì´ë¸” ì •ì˜ì—ì„œ role ì— ë”°ë¼ ì–´ë–¤ ì»¬ëŸ¼ì´ data/metadata ì¸ì§€ í™•ì¸
                target_tables = {t["name"]: t for t in spec.get("target_tables", [])}
                roles_by_table: Dict[str, Dict[str, str]] = {}
                for tname, tdef in target_tables.items():
                    roles = {}
                    for col in tdef.get("columns", []):
                        roles[col["name"]] = col.get("role", "data")
                    roles_by_table[tname] = roles

                # íŒŒì¼ë³„/í…Œì´ë¸”ë³„ CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                for file_entry in all_files:
                    fname = file_entry["file_name"]
                    if fname not in outputs:
                        continue
                    st.markdown(f"### ğŸ“ {fname} â€” ë³€í™˜ëœ í…Œì´ë¸”")

                    for table_name, df_canon in outputs[fname].items():
                        st.markdown(f"#### ğŸ“Š í…Œì´ë¸”: `{table_name}`")

                        # role ê¸°ë°˜ìœ¼ë¡œ CSVì— í¬í•¨í•  ì»¬ëŸ¼ ê²°ì •
                        roles = roles_by_table.get(table_name, {})
                        data_cols = [
                            c
                            for c in df_canon.columns
                            if c in roles and roles.get(c, "data") in ("data", "both")
                        ]
                        # í•­ìƒ í¬í•¨í•  ì¶œì²˜ ì»¬ëŸ¼
                        extra_cols = ["_source_file", "_source_sheet"]
                        csv_cols = data_cols + [c for c in extra_cols if c in df_canon.columns]

                        if not csv_cols:
                            st.info("ì´ í…Œì´ë¸”ì—ì„œ CSVì— í¬í•¨í•  ë°ì´í„° ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                            continue

                        df_csv = df_canon[csv_cols].copy()
                        st.dataframe(df_csv.head(20))

                        csv_bytes = df_csv.to_csv(index=False).encode("utf-8-sig")
                        safe_fname = os.path.splitext(os.path.basename(fname))[0]
                        out_name = f"{safe_fname}__{table_name}.csv"

                        st.download_button(
                            label=f"â¬‡ï¸ {out_name} ë‹¤ìš´ë¡œë“œ",
                            data=csv_bytes,
                            file_name=out_name,
                            mime="text/csv",
                        )

                # metadata.json ë‹¤ìš´ë¡œë“œ
                st.markdown("### ğŸ§¾ metadata.json")
                st.code(json.dumps(metadata, ensure_ascii=False, indent=2)[:6000], language="json")
                meta_bytes = json.dumps(metadata, ensure_ascii=False, indent=2).encode("utf-8")
                st.download_button(
                    label="â¬‡ï¸ metadata.json ë‹¤ìš´ë¡œë“œ",
                    data=meta_bytes,
                    file_name="metadata.json",
                    mime="application/json",
                )


if __name__ == "__main__":
    main()
